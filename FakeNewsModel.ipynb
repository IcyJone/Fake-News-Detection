{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jones\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jones\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jones\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# download necessary nltk data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load datasets\n",
    "df_fake = pd.read_csv(r\"\\Fake.csv\")\n",
    "df_true = pd.read_csv(r\"\\True.csv\")\n",
    "\n",
    "# Label datasets\n",
    "df_true['status'] = 0\n",
    "df_fake['status'] = 1\n",
    "\n",
    "# Merge datasets\n",
    "df = pd.concat([df_true, df_fake]).reset_index(drop=True)\n",
    "\n",
    "# Retain only the title and label\n",
    "df = df[['title', 'status']]\n",
    "\n",
    "# Shuffle data\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Check for null values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    # Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_title'] = df['title'].apply(preprocess_text)\n",
    "\n",
    "# Check maximum sequence length\n",
    "df['title_length'] = df['cleaned_title'].apply(lambda x: len(x.split()))\n",
    "max_length = max(df['title_length'])\n",
    "\n",
    "# Word embedding\n",
    "def word_embedding(text, vocab_size=5000, max_length=40):\n",
    "    encoded = one_hot(text, vocab_size)\n",
    "    padded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "    return padded[0]\n",
    "\n",
    "# Prepare data for training\n",
    "vocab_size = 5000\n",
    "max_length = 40\n",
    "df['embedded_title'] = df['cleaned_title'].apply(lambda x: word_embedding(x, vocab_size, max_length))\n",
    "X = np.array(df['embedded_title'].tolist())\n",
    "y = df['status'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create LSTM model\n",
    "embedded_features = 40\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedded_features, input_length=max_length),\n",
    "    LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 24ms/step - accuracy: 0.8525 - loss: 0.3423 - val_accuracy: 0.9302 - val_loss: 0.1669\n",
      "Epoch 2/10\n",
      "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 21ms/step - accuracy: 0.9539 - loss: 0.1230 - val_accuracy: 0.9410 - val_loss: 0.1519\n",
      "Epoch 3/10\n",
      "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.9635 - loss: 0.0951 - val_accuracy: 0.9419 - val_loss: 0.1586\n",
      "Epoch 4/10\n",
      "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 20ms/step - accuracy: 0.9733 - loss: 0.0756 - val_accuracy: 0.9393 - val_loss: 0.1710\n",
      "Epoch 5/10\n",
      "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 21ms/step - accuracy: 0.9768 - loss: 0.0644 - val_accuracy: 0.9385 - val_loss: 0.1853\n",
      "Epoch 6/10\n",
      "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 21ms/step - accuracy: 0.9805 - loss: 0.0556 - val_accuracy: 0.9382 - val_loss: 0.1952\n",
      "Epoch 7/10\n",
      "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 21ms/step - accuracy: 0.9834 - loss: 0.0464 - val_accuracy: 0.9400 - val_loss: 0.2122\n",
      "Epoch 8/10\n",
      "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 21ms/step - accuracy: 0.9869 - loss: 0.0381 - val_accuracy: 0.9391 - val_loss: 0.2233\n",
      "Epoch 9/10\n",
      "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 27ms/step - accuracy: 0.9890 - loss: 0.0302 - val_accuracy: 0.9282 - val_loss: 0.2742\n",
      "Epoch 10/10\n",
      "\u001b[1m562/562\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 25ms/step - accuracy: 0.9896 - loss: 0.0290 - val_accuracy: 0.9329 - val_loss: 0.2887\n",
      "\n",
      "Model Evaluation\n",
      "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
      "Confusion Matrix:\n",
      " [[3918  366]\n",
      " [ 247 4449]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93      4284\n",
      "           1       0.92      0.95      0.94      4696\n",
      "\n",
      "    accuracy                           0.93      8980\n",
      "   macro avg       0.93      0.93      0.93      8980\n",
      "weighted avg       0.93      0.93      0.93      8980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64)\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model():\n",
    "    print(\"\\nModel Evaluation\")\n",
    "    y_pred = (model.predict(X_test) > 0.4).astype(int).flatten()\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "evaluate_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Prediction: Fake News\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function for real-time prediction\n",
    "def predict_news(text):\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    embedded_text = word_embedding(cleaned_text, vocab_size, max_length)\n",
    "    prediction = model.predict(np.array([embedded_text]))\n",
    "    return 'Fake News' if prediction[0][0] > 0.4 else 'Real News'\n",
    "\n",
    "# Example prediction\n",
    "example_news = \"Former CIA Director Slams Trump Over UN Bullying, Openly Suggests He’s Acting Like A Dictator (TWEET)\"\n",
    "print(\"Prediction:\", predict_news(example_news))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
